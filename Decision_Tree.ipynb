{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ques 1) What is a Decision Tree, and how does it work in the context of classification? **\n",
        "\n",
        "**Ans)**   A **Decision Tree** is a popular supervised machine learning algorithm used for both classification and regression tasks, but it is most commonly associated with classification problems. It works by splitting the dataset into subsets based on the value of input features, creating a tree-like model of decisions. Each internal node in the tree represents a test on a feature (e.g., \"Is age > 30?\"), each branch represents the outcome of the test, and each leaf node represents a class label or decision. The tree is built by recursively selecting the feature that best splits the data according to a certain criterion, such as **Gini impurity** or **information gain**. This process continues until a stopping condition is met, such as a maximum tree depth or a minimum number of samples per leaf. The resulting model can then classify new data points by traversing the tree from the root to a leaf, following the decisions at each node based on the input features. Decision Trees are intuitive, easy to interpret, and handle both numerical and categorical data, though they can be prone to overfitting if not properly pruned or regularized.\n",
        "\n",
        "\n",
        "\n",
        "# **Ques 2) Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree? **\n",
        "\n",
        "**Ans)- **    **Gini Impurity** and **Entropy** are two commonly used impurity measures that help decision trees determine the best feature to split the data at each node. These measures evaluate how \"pure\" or \"impure\" a node is — meaning how mixed the class labels are within that node.\n",
        "\n",
        "**Gini Impurity** measures the likelihood of an incorrect classification of a randomly chosen element if it was randomly labeled according to the distribution of labels in the node. Its formula is:\n",
        "\n",
        "$$\n",
        "\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "\n",
        "where $p_i$ is the probability of class $i$ in the node, and $C$ is the total number of classes. A Gini Impurity of 0 means the node is pure (all instances belong to one class), and higher values indicate more mixed classes.\n",
        "\n",
        "**Entropy**, derived from information theory, measures the amount of uncertainty or disorder in a node. Its formula is:\n",
        "\n",
        "$$\n",
        "\\text{Entropy} = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "Similar to Gini, an entropy of 0 means the node is pure. Entropy increases as the class distribution becomes more uniform.\n",
        "\n",
        "When building a decision tree, the algorithm evaluates all possible splits for each feature and selects the one that results in the greatest **reduction in impurity**—known as **information gain** (for entropy) or **Gini gain**. The goal is to produce child nodes that are as pure as possible. Although both Gini and Entropy often lead to similar splits, Gini tends to be slightly faster to compute and may prefer splits that isolate the most frequent class, while Entropy is more sensitive to changes in class distribution. The choice between them depends on the specific problem, but both are effective for guiding tree growth.\n",
        "\n",
        "\n",
        "\n",
        "# **Ques 3) What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "\n",
        "**Ans)- ** **Pre-pruning** and **Post-pruning** are two techniques used to prevent **overfitting** in decision trees by controlling their complexity.\n",
        "\n",
        "**Pre-pruning** (also known as *early stopping*) involves halting the tree’s growth during the construction phase. It sets constraints like a maximum tree depth, minimum number of samples required to split a node, or minimum information gain needed for a split. If any of these conditions are not met, the node becomes a leaf and no further splitting is done.\n",
        "\n",
        "* *Practical Advantage:* It reduces training time and keeps the model simpler and faster, which is especially useful for large datasets or real-time applications.\n",
        "\n",
        "**Post-pruning** (also called *cost complexity pruning*) allows the tree to grow fully and then removes branches that have little or no impact on prediction accuracy, often based on performance on a validation set.\n",
        "\n",
        "* *Practical Advantage:* It typically results in a more accurate model, as it allows the tree to consider all possible splits first and then eliminate only those that lead to overfitting.\n",
        "\n",
        "\n",
        "# **Ques 4) What is Information Gain in Decision Trees, and why is it important for choosing the best split? **\n",
        "\n",
        "\n",
        "**Ans)-**  **Information Gain** is a key concept in decision trees used to determine the best feature and threshold to split the data at each node. It measures how much \"information\" or **reduction in uncertainty** is gained about the target variable after the dataset is split based on a particular feature.\n",
        "\n",
        "Mathematically, **Information Gain (IG)** is calculated as:\n",
        "\n",
        "$$\n",
        "\\text{IG} = \\text{Entropy (parent)} - \\sum_{i=1}^{k} \\frac{n_i}{n} \\times \\text{Entropy (child}_i\\text{)}\n",
        "$$\n",
        "\n",
        "where $n_i$ is the number of samples in child node $i$, and $n$ is the total number of samples in the parent node. The idea is to compare the entropy (impurity) before the split and after the split. A higher information gain means a more effective split in terms of class separation.\n",
        "\n",
        "**Why it's important:** Information Gain helps the decision tree algorithm choose the feature that provides the **most \"pure\" subgroups**, effectively reducing the randomness in the classification task. By maximizing information gain at each node, the tree becomes more accurate and efficient in learning patterns from the data. Without a metric like information gain, the tree would have no systematic way of deciding how to divide the data, leading to poor or arbitrary splits.\n",
        "\n",
        "\n",
        "\n",
        "# **Ques 5) What are some common real-world applications of Decision Trees, and what are their main advantages and limitations? **\n",
        "\n",
        "**Ans)-**  **Decision Trees** are widely used in various real-world applications because of their simplicity, interpretability, and ability to handle both numerical and categorical data. Here are some common applications, along with their key **advantages** and **limitations**:\n",
        "\n",
        "---\n",
        "\n",
        "### **Real-World Applications:**\n",
        "\n",
        "1. **Medical Diagnosis:**\n",
        "   Decision trees help diagnose diseases by evaluating patient symptoms and medical history to arrive at a likely condition (e.g., diagnosing diabetes or heart disease).\n",
        "\n",
        "2. **Loan Approval and Credit Scoring:**\n",
        "   Banks use decision trees to assess whether a loan applicant is likely to repay based on income, credit history, and other financial factors.\n",
        "\n",
        "3. **Customer Churn Prediction:**\n",
        "   Businesses use them to predict whether a customer is likely to stop using a service, based on usage patterns and engagement metrics.\n",
        "\n",
        "4. **Fraud Detection:**\n",
        "   Decision trees are used in identifying fraudulent transactions by spotting patterns that deviate from normal behavior.\n",
        "\n",
        "5. **Marketing and Recommendation Systems:**\n",
        "   They help in segmenting customers and tailoring promotions based on purchasing behavior and preferences.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages:**\n",
        "\n",
        "* **Easy to Understand and Interpret:**\n",
        "  The tree structure makes it simple for humans to follow the logic behind predictions.\n",
        "\n",
        "* **Handles Both Types of Data:**\n",
        "  Works well with categorical and numerical features.\n",
        "\n",
        "* **No Need for Feature Scaling:**\n",
        "  Unlike algorithms like SVM or k-NN, decision trees don't require normalization or standardization.\n",
        "\n",
        "* **Fast and Efficient:**\n",
        "  Relatively quick to train and predict, even with large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations:**\n",
        "\n",
        "* **Overfitting:**\n",
        "  Trees can become too complex and fit the noise in the data unless properly pruned.\n",
        "\n",
        "* **Instability:**\n",
        "  Small changes in the data can lead to a completely different tree structure.\n",
        "\n",
        "* **Biased with Imbalanced Data:**\n",
        "  Decision trees may favor classes with more instances unless techniques like class weighting or resampling are used.\n",
        "\n",
        "* **Less Accurate Alone:**\n",
        "  While easy to interpret, a single decision tree might not be as accurate as ensemble methods like **Random Forests** or **Gradient Boosted Trees**.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aGEfdtPtkYfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1MYSmlnj_FG",
        "outputId": "f2aacdf8-4c8d-4967-cf86-0c0b65c744bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ],
      "source": [
        "# Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV). ● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "# Question 6:   Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier using the Gini criterion ● Print the model’s accuracy and feature importances\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 7 Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pflf12J6mZNH",
        "outputId": "070bc58a-1dff-4ed0-c671-34bf04ece653"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 8 Write a Python program to: ● Load the California Housing dataset from sklearn ● Train a Decision Tree Regressor ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cPRQnLxm0PL",
        "outputId": "62f95340-7459-498d-dc67-3ba2c3c96c00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 9 Write a Python program to: ● Load the Iris Dataset ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on test set using best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy with Best Parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWbTyeDIm61t",
        "outputId": "d5f4247b-b91a-4c97-a435-7d87414061b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Test Set Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ques 10) Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease.\n",
        "You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance And describe what business value this model could provide in the real-world setting."
      ],
      "metadata": {
        "id": "hryMxOj3nVty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Ques 10) Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease.\n",
        "#You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "# ● Evaluate its performance And describe what business value this model could provide in the real-world setting. *\n",
        "\n",
        "\n",
        "\n",
        "**Ans)- ** Absolutely! Here’s a detailed step-by-step process for building a Decision Tree model in a healthcare setting, starting from messy data and ending with a valuable, deployable model:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Handle Missing Values**\n",
        "\n",
        "* **Understand the Data:**\n",
        "  First, analyze which features have missing values and how much data is missing. Is it random or systematic?\n",
        "\n",
        "* **Imputation Strategies:**\n",
        "\n",
        "  * For **numerical features**, consider imputing missing values using techniques like:\n",
        "\n",
        "    * Mean or median imputation (simple and fast)\n",
        "    * More advanced methods like K-Nearest Neighbors (KNN) or iterative imputation if missingness is significant\n",
        "  * For **categorical features**, replace missing values with the most frequent category or create a new category like “Unknown”.\n",
        "\n",
        "* **Consider Dropping:**\n",
        "  If a feature has too many missing values and is not critical, it might be better to drop it to avoid noise.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Encode Categorical Features**\n",
        "\n",
        "* **Identify Categorical Variables:**\n",
        "  Check which columns are categorical (e.g., gender, ethnicity, symptom categories).\n",
        "\n",
        "* **Encoding Methods:**\n",
        "\n",
        "  * For **ordinal categories** (where order matters), use **Ordinal Encoding**.\n",
        "  * For **nominal categories** (no order), use **One-Hot Encoding** to convert categories into binary columns.\n",
        "  * Some decision tree implementations can handle categorical data natively, but scikit-learn’s DecisionTreeClassifier requires numeric input, so encoding is necessary.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Train a Decision Tree Model**\n",
        "\n",
        "* **Split the Data:**\n",
        "  Divide your dataset into training and testing sets (e.g., 80%-20%) to evaluate performance on unseen data.\n",
        "\n",
        "* **Train the Model:**\n",
        "  Use a DecisionTreeClassifier from libraries like scikit-learn, setting an initial criterion like Gini impurity.\n",
        "\n",
        "* **Handle Class Imbalance (if present):**\n",
        "  Diseases can be rare, so if the classes are imbalanced, consider:\n",
        "\n",
        "  * Using class weights to penalize misclassification of minority classes more heavily\n",
        "  * Applying resampling techniques (oversampling minority or undersampling majority)\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Tune Hyperparameters**\n",
        "\n",
        "* **Parameters to Tune:**\n",
        "\n",
        "  * `max_depth` (controls tree depth to prevent overfitting)\n",
        "  * `min_samples_split` (minimum samples required to split a node)\n",
        "  * `min_samples_leaf` (minimum samples at a leaf node)\n",
        "  * `max_features` (number of features considered for splitting)\n",
        "* **Use GridSearchCV or RandomizedSearchCV:**\n",
        "  Automate the search for the best combination of hyperparameters using cross-validation to avoid overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Evaluate Performance**\n",
        "\n",
        "* **Metrics:**\n",
        "  Since this is a classification task, and especially in healthcare, focus on:\n",
        "\n",
        "  * **Accuracy** (overall correctness)\n",
        "  * **Precision** (how many predicted positives are true positives)\n",
        "  * **Recall / Sensitivity** (how many actual positives were caught)\n",
        "  * **F1 Score** (balance between precision and recall)\n",
        "  * **ROC-AUC** (overall ability to discriminate between classes)\n",
        "\n",
        "* **Confusion Matrix:**\n",
        "  Analyze false positives and false negatives carefully — in healthcare, false negatives can be especially costly.\n",
        "\n",
        "* **Validation:**\n",
        "  Use cross-validation to ensure robustness and possibly test on a completely separate hold-out dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **Business Value of the Model**\n",
        "\n",
        "* **Early Disease Detection:**\n",
        "  The model can flag high-risk patients early, enabling timely intervention that can save lives and reduce treatment costs.\n",
        "\n",
        "* **Resource Optimization:**\n",
        "  Helps prioritize medical tests and appointments for patients more likely to have the disease, improving healthcare efficiency.\n",
        "\n",
        "* **Personalized Care:**\n",
        "  Enables healthcare providers to tailor treatments based on predicted risk, improving patient outcomes.\n",
        "\n",
        "* **Cost Reduction:**\n",
        "  By avoiding unnecessary procedures for low-risk patients, it reduces healthcare expenses.\n",
        "\n",
        "* **Data-Driven Decisions:**\n",
        "  Empowers clinicians with insights from data, supporting better clinical decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "This end-to-end approach ensures the model is reliable, interpretable, and actionable, driving real impact in patient care and healthcare management. Would you like me to help with the actual code implementation of these steps?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uhsELVFEneLE"
      }
    }
  ]
}